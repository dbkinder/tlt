<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>tlt.models.hf_model &mdash; Intel® Transfer Learning Tool 0.2.0 documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/tlt-custom.css" type="text/css" />
    <link rel="shortcut icon" href="../../../_static/favicon-intel-32x32.png"/>
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/sphinx_highlight.js"></script>
        <script src="../../../_static/tlt-custom.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            Intel® Transfer Learning Tool
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../index.html">Documentation Home</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../GetStarted.html">Get Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../examples/examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cli.html">CLI Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Models.html">Supported Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Legal.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../genindex.html">Index</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/IntelAI/transfer-learning-tool">GitHub Repository</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Intel® Transfer Learning Tool</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">tlt.models.hf_model</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for tlt.models.hf_model</h1><div class="highlight"><pre>
<span></span><span class="ch">#!/usr/bin/env python</span>
<span class="c1"># -*- coding: utf-8 -*-</span>
<span class="c1">#</span>
<span class="c1"># Copyright (c) 2022 Intel Corporation</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#    http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1">#</span>
<span class="c1"># SPDX-License-Identifier: Apache-2.0</span>
<span class="c1">#</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">neural_compressor</span> <span class="kn">import</span> <span class="n">quantization</span>
<span class="kn">from</span> <span class="nn">neural_compressor.config</span> <span class="kn">import</span> <span class="n">BenchmarkConfig</span>

<span class="kn">from</span> <span class="nn">tlt.models.model</span> <span class="kn">import</span> <span class="n">BaseModel</span>
<span class="kn">from</span> <span class="nn">tlt.utils.types</span> <span class="kn">import</span> <span class="n">FrameworkType</span><span class="p">,</span> <span class="n">UseCaseType</span>
<span class="kn">from</span> <span class="nn">tlt.utils.file_utils</span> <span class="kn">import</span> <span class="n">verify_directory</span>
<span class="kn">from</span> <span class="nn">tlt.utils.inc_utils</span> <span class="kn">import</span> <span class="n">get_inc_config</span>


<div class="viewcode-block" id="HFModel"><a class="viewcode-back" href="../../../_autosummary/tlt.models.hf_model.HFModel.html#tlt.models.hf_model.HFModel">[docs]</a><span class="k">class</span> <span class="nc">HFModel</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class to represent a Hugging Face model</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="HFModel.__init__"><a class="viewcode-back" href="../../../_autosummary/tlt.models.hf_model.HFModel.html#tlt.models.hf_model.HFModel.__init__">[docs]</a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">framework</span><span class="p">:</span> <span class="n">FrameworkType</span><span class="p">,</span> <span class="n">use_case</span><span class="p">:</span> <span class="n">UseCaseType</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">framework</span><span class="p">,</span> <span class="n">use_case</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_history</span> <span class="o">=</span> <span class="p">{}</span></div>

    <span class="k">def</span> <span class="nf">_update_history</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_history</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_history</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_history</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">value</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">_check_optimizer_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">optimizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="n">inspect</span><span class="o">.</span><span class="n">isclass</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span> <span class="ow">or</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getmro</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The optimizer input must be a class (not an instance) of type torch.optim.Optimizer or &quot;</span>
                            <span class="s2">&quot;None but found a </span><span class="si">{}</span><span class="s2">. Example: torch.optim.AdamW&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)))</span>
        <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="n">inspect</span><span class="o">.</span><span class="n">isclass</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="ow">or</span>
                                 <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">_Loss</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getmro</span><span class="p">(</span><span class="n">loss</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The optimizer input must be a class (not an instance) of type &quot;</span>
                            <span class="s2">&quot;torch.nn.modules.loss._Loss or None but found a </span><span class="si">{}</span><span class="s2">. &quot;</span>
                            <span class="s2">&quot;Example: torch.nn.CrossEntropyLoss&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">loss</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">_check_train_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dir</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">dataset_type</span><span class="p">,</span> <span class="n">extra_layers</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">distributed</span><span class="p">,</span> <span class="n">hostfile</span><span class="p">):</span>
        <span class="n">verify_directory</span><span class="p">(</span><span class="n">output_dir</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">distributed</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">hostfile</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span> <span class="n">hostfile</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">FileNotFoundError</span><span class="p">(</span><span class="s2">&quot;Could not find hostfile. Consider creating one&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">dataset_type</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The dataset must be a </span><span class="si">{}</span><span class="s2"> but found a </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dataset_type</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">dataset</span><span class="p">)))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">dataset</span><span class="o">.</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;preprocessing_info&#39;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Dataset hasn&#39;t been preprocessed yet.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">extra_layers</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">extra_layers</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">extra_layers</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;extra_layers argument must be a list of integers but found a </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">extra_layers</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Invalid type for the epochs arg. Expected an int but found a </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">epochs</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">optimize_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dir</span><span class="p">,</span> <span class="n">overwrite_model</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Performs FP32 graph optimization using the Intel Neural Compressor on the model</span>
<span class="sd">        and writes the inference-optimized model to the output_dir. Graph optimization includes converting</span>
<span class="sd">        variables to constants, removing training-only operations like checkpoint saving, stripping out parts</span>
<span class="sd">        of the graph that are never reached, removing debug operations like CheckNumerics, folding batch</span>
<span class="sd">        normalization ops into the pre-calculated weights, and fusing common operations into unified versions.</span>
<span class="sd">        Args:</span>
<span class="sd">            output_dir (str): Writable output directory to save the optimized model</span>
<span class="sd">            overwrite_model (bool): Specify whether or not to overwrite the output_dir, if it already exists</span>
<span class="sd">                                    (default: False)</span>
<span class="sd">        Returns:</span>
<span class="sd">            None</span>
<span class="sd">        Raises:</span>
<span class="sd">            NotImplementedError: because this hasn&#39;t been implemented yet for PyTorch</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Only TensorFlow graph optimization is currently supported by the &quot;</span>
                                  <span class="s2">&quot;Intel Neural Compressor (INC)&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">quantize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dir</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">overwrite_model</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Performs post training quantization using the Intel Neural Compressor on the model using the dataset.</span>
<span class="sd">        The dataset&#39;s training subset will be used as the calibration data and its validation or test subset will</span>
<span class="sd">        be used for evaluation. The quantized model is written to the output directory.</span>

<span class="sd">        Args:</span>
<span class="sd">            output_dir (str): Writable output directory to save the quantized model</span>
<span class="sd">            dataset (ImageClassificationDataset): dataset to quantize with</span>
<span class="sd">            config (PostTrainingQuantConfig): Optional, for customizing the quantization parameters</span>
<span class="sd">            overwrite_model (bool): Specify whether or not to overwrite the output_dir, if it already exists</span>
<span class="sd">                                    (default: False)</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Raises:</span>
<span class="sd">            FileExistsError: if the output_dir already has a model.pt file</span>
<span class="sd">            ValueError: if the dataset is not compatible for quantizing the model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">output_dir</span><span class="p">):</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">output_dir</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Verify that the output directory doesn&#39;t already have a model.pt or best_model.pt file</span>
            <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="s2">&quot;model.pt&quot;</span><span class="p">))</span> <span class="ow">or</span> \
                    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="s2">&quot;best_model.pt&quot;</span><span class="p">)):</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">overwrite_model</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">FileExistsError</span><span class="p">(</span><span class="s2">&quot;A saved model already exists in: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">output_dir</span><span class="p">))</span>

        <span class="c1"># Verify dataset is of the right type</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inc_compatible_dataset</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Quantization is compatible with datasets of type </span><span class="si">{}</span><span class="s1">, and type &#39;</span>
                             <span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> was found&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_inc_compatible_dataset</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">dataset</span><span class="p">)))</span>

        <span class="n">config</span> <span class="o">=</span> <span class="n">config</span> <span class="k">if</span> <span class="n">config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">get_inc_config</span><span class="p">(</span><span class="n">approach</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_quantization_approach</span><span class="p">)</span>

        <span class="n">calib_dataloader</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">get_inc_dataloaders</span><span class="p">()</span>
        <span class="n">config</span><span class="o">.</span><span class="n">backend</span> <span class="o">=</span> <span class="s1">&#39;default&#39;</span>
        <span class="n">quantized_model</span> <span class="o">=</span> <span class="n">quantization</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="p">,</span> <span class="n">conf</span><span class="o">=</span><span class="n">config</span><span class="p">,</span> <span class="n">calib_dataloader</span><span class="o">=</span><span class="n">calib_dataloader</span><span class="p">)</span>

        <span class="c1"># If quantization was successful, save the model</span>
        <span class="k">if</span> <span class="n">quantized_model</span><span class="p">:</span>
            <span class="n">quantized_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">output_dir</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="s1">&#39;best_model.pt&#39;</span><span class="p">)):</span>
                <span class="c1"># Change the model filename from best_model.pt to model.pt to match our convention</span>
                <span class="n">os</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="s1">&#39;best_model.pt&#39;</span><span class="p">),</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="s1">&#39;model.pt&#39;</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;There was an error with quantization&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">benchmark</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">saved_model_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">warmup</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">iteration</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">cores_per_instance</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">num_of_instance</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inter_num_of_threads</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">intra_num_of_threads</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Use Intel Neural Compressor to benchmark the model with the dataset argument. The dataset&#39;s validation or test</span>
<span class="sd">        subset will be used for benchmarking, if present. Otherwise, the full training dataset is used. The model to be</span>
<span class="sd">        benchmarked can also be explicitly set to a saved_model_dir containing for example a quantized saved model.</span>

<span class="sd">        Args:</span>
<span class="sd">            dataset (ImageClassificationDataset): Dataset to use for benchmarking</span>
<span class="sd">            saved_model_dir (str): Optional, path to the directory where the saved model is located</span>
<span class="sd">            warmup (int): The number of iterations to perform before running performance tests, default is 10</span>
<span class="sd">            iteration (int): The number of iterations to run performance tests, default is 100</span>
<span class="sd">            cores_per_instance (int or None): The number of CPU cores to use per instance, default is None</span>
<span class="sd">            num_of_instance (int or None): The number of instances to use for performance testing, default is None</span>
<span class="sd">            inter_num_of_threads (int or None): The number of threads to use for inter-thread operations, default is</span>
<span class="sd">                                                None</span>
<span class="sd">            intra_num_of_threads (int or None): The number of threads to use for intra-thread operations, default is</span>
<span class="sd">                                                None</span>

<span class="sd">        Returns:</span>
<span class="sd">            Benchmarking results from Intel Neural Compressor</span>

<span class="sd">        Raises:</span>
<span class="sd">            NotADirectoryError: if the saved_model_dir is not None or a valid directory</span>
<span class="sd">            FileNotFoundError: if a model.pt is not found in the saved_model_dir or if the inc_config_path file</span>
<span class="sd">            is not found</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Verify dataset is of the right type</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inc_compatible_dataset</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;Quantization has only been implemented for TLT datasets, and type &#39;</span>
                                      <span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> was found&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">dataset</span><span class="p">)))</span>

        <span class="c1"># If provided, the saved model directory should exist and contain a model.pt file</span>
        <span class="k">if</span> <span class="n">saved_model_dir</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">saved_model_dir</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">NotADirectoryError</span><span class="p">(</span><span class="s2">&quot;The saved model directory (</span><span class="si">{}</span><span class="s2">) does not exist.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">saved_model_dir</span><span class="p">))</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">saved_model_dir</span><span class="p">,</span> <span class="s2">&quot;model.pt&quot;</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">FileNotFoundError</span><span class="p">(</span><span class="s2">&quot;The saved model directory (</span><span class="si">{}</span><span class="s2">) should have a model.pt file&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">saved_model_dir</span><span class="p">))</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">saved_model_dir</span><span class="p">,</span> <span class="s1">&#39;model.pt&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span>

        <span class="n">config</span> <span class="o">=</span> <span class="n">BenchmarkConfig</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s2">&quot;ipex&quot;</span><span class="p">,</span> <span class="n">warmup</span><span class="o">=</span><span class="n">warmup</span><span class="p">,</span> <span class="n">iteration</span><span class="o">=</span><span class="n">iteration</span><span class="p">,</span>
                                 <span class="n">cores_per_instance</span><span class="o">=</span><span class="n">cores_per_instance</span><span class="p">,</span> <span class="n">num_of_instance</span><span class="o">=</span><span class="n">num_of_instance</span><span class="p">,</span>
                                 <span class="n">inter_num_of_threads</span><span class="o">=</span><span class="n">inter_num_of_threads</span><span class="p">,</span> <span class="n">intra_num_of_threads</span><span class="o">=</span><span class="n">intra_num_of_threads</span><span class="p">)</span>

        <span class="n">_</span><span class="p">,</span> <span class="n">eval_dataloader</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">get_inc_dataloaders</span><span class="p">()</span>

        <span class="kn">from</span> <span class="nn">neural_compressor.benchmark</span> <span class="kn">import</span> <span class="n">fit</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span> <span class="n">b_dataloader</span><span class="o">=</span><span class="n">eval_dataloader</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">AssertionError</span><span class="p">:</span>
            <span class="c1"># Use INC&#39;s special load utility to reload an int8 ipex model</span>
            <span class="kn">from</span> <span class="nn">neural_compressor.utils.pytorch</span> <span class="kn">import</span> <span class="n">load</span>
            <span class="n">quantized_model</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="p">,</span> <span class="n">dataloader</span><span class="o">=</span><span class="n">eval_dataloader</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">quantized_model</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span> <span class="n">b_dataloader</span><span class="o">=</span><span class="n">eval_dataloader</span><span class="p">)</span></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022-2023, Intel Corporation.
      <span class="lastupdated">Last updated on Jun 12, 2023.
      </span></p>
  </div>

  
*Other names and brands may be claimed as the property of others.
<a href="http://www.intel.com/content/www/us/en/legal/trademarks.html">Trademarks</a>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>